{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa44a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import photutils\n",
    "\n",
    "from skimage import data, img_as_float\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import mean_squared_error\n",
    "\n",
    "from photutils.detection import DAOStarFinder\n",
    "from photutils.morphology import data_properties\n",
    "from astropy.stats import mad_std\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Lambda, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import mse\n",
    "\n",
    "from tensorflow.config import list_logical_devices, list_physical_devices, set_visible_devices\n",
    "from tensorflow.config.experimental import set_memory_growth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a4bf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup notebook so Tensorflow works in a friendly way in a shared multi GPU env\n",
    "\n",
    "def setup_tensorflow(seed=None):\n",
    "    try:\n",
    "        import nvsmi\n",
    "\n",
    "        gpu_mem_free = np.array([gpu.mem_free for gpu in nvsmi.get_gpus()])\n",
    "        if len(gpu_mem_free) > 0 and (gpu_mem_free > 0).any():\n",
    "            idx = np.argmax(gpu_mem_free)\n",
    "            gpu = list_physical_devices(\"GPU\")[idx]\n",
    "            set_memory_growth(gpu, True)\n",
    "            set_visible_devices(gpu, \"GPU\")\n",
    "            gpus = list_logical_devices(\"GPU\")\n",
    "            print(f\"Using GPU: {gpus}\")\n",
    "        else:\n",
    "            raise Exception\n",
    "    except:\n",
    "        print(f\"Tensorflow using CPU\")\n",
    "    tf.random.set_seed(seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cfcff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_tensorflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f611ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load the image arrays \n",
    "\n",
    "blends = np.load(\"blends.npy\")\n",
    "components = np.load(\"components.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0e0378",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into training and test set \n",
    "\n",
    "#Going to split training and test 80% and 20% respectively \n",
    "\n",
    "train_ngals = int(blends.shape[0] * 0.8)\n",
    "test_ngals = int(blends.shape[0] * 0.2)\n",
    "\n",
    "train_blends = blends[:train_ngals]\n",
    "test_blends = blends[train_ngals:]\n",
    "\n",
    "a, b, c, d, test_components = np.split(components, 5, axis = 1)\n",
    "train_components = np.concatenate((a, b, c, d), axis = 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e6f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "blends.shape, train_blends.shape, test_blends.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cd3a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "components.shape, train_components.shape, test_components.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c306126",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Normalise the data\n",
    "\n",
    "#Blends\n",
    "\n",
    "train_blends_max = np.amax(train_blends)\n",
    "train_blends_min = np.amin(train_blends)\n",
    "\n",
    "train_blends = (train_blends - train_blends_min) / (train_blends_max - train_blends_min) \n",
    "\n",
    "test_blends_max = np.amax(test_blends)\n",
    "test_blends_min = np.amin(test_blends)\n",
    "\n",
    "test_blends = (test_blends - test_blends_min) / (test_blends_max - test_blends_min) \n",
    "\n",
    "#Components\n",
    "\n",
    "train_components_max = np.amax(train_components) \n",
    "train_components_min = np.amin(train_components)\n",
    "\n",
    "train_components = (train_components - train_components_min) / (train_components_max - train_components_min) \n",
    "\n",
    "test_components_max = np.amax(test_components) \n",
    "test_components_min = np.amin(test_components)\n",
    "\n",
    "test_components = (test_components - test_components_min) / (test_components_max - test_components_min) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9635daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.amax(train_blends), np.amin(train_blends), np.amax(test_blends), np.amin(test_blends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe97408",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.amax(test_components), np.amin(test_components), np.amax(test_blends), np.amin(test_blends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4868fab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "height,width=64,64\n",
    "\n",
    "train_blends = train_blends.reshape(train_ngals, height, width, 1).astype('float32')\n",
    "train_components = train_components.reshape(2, train_ngals, height, width, 1).astype('float32')\n",
    "\n",
    "test_blends = test_blends.reshape(test_ngals, height, width, 1).astype('float32')\n",
    "test_components = test_components.reshape(2, test_ngals, height, width, 1).astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c57d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "latent_dimensions = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4916edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "blends.shape, components.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb8e636",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2 + 1, 5, figsize=(15, 10))\n",
    "for i in range(5):\n",
    "    ax[0, i].imshow(train_blends[i], origin='lower', interpolation='nearest', vmin = 0, vmax = 0.75)\n",
    "    ax[1, i].imshow(train_components[0, i], origin='lower', interpolation='nearest', vmin = 0, vmax = 0.75)\n",
    "    ax[2, i].imshow(train_components[1, i], origin='lower', interpolation='nearest', vmin = 0, vmax = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acfb71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Will optimise encoder / decoder later\n",
    "def encoder(x):\n",
    "    x=Conv2D(32,3,activation='relu', strides = 2, padding='same')(x)\n",
    "    x=Conv2D(64,3,activation='relu', strides = 2, padding='same')(x)\n",
    "    x=MaxPooling2D(pool_size=(2,2))(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17bc7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(x):\n",
    "    x=UpSampling2D(size=(2,2))(x)\n",
    "    x=Conv2DTranspose(64,3,activation='relu', strides = 2 ,padding='same')(x)\n",
    "    x=Conv2DTranspose(32,3,activation='relu', strides = 2 ,padding='same')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc61a107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean,z_log_sigma=args\n",
    "    epsilon=tf.random.normal(shape=tf.shape(z_mean))\n",
    "    z=z_mean+tf.exp(0.5*z_log_sigma)*epsilon\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d8cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "input_blend=Input(shape=(height, width,1))\n",
    "\n",
    "encoded=encoder(input_blend)\n",
    "shape=encoded.get_shape()[1:]\n",
    "encoded=Flatten()(encoded)\n",
    "\n",
    "z_mean=Dense(latent_dimensions)(encoded)\n",
    "z_log_sigma=Dense(latent_dimensions)(encoded)\n",
    "z=Lambda(sampling)([z_mean,z_log_sigma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c2d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder\n",
    "input_embed=Input(shape=latent_dimensions)\n",
    "\n",
    "embed1=Dense(np.prod(shape),activation='relu')(input_embed)\n",
    "embed1=Reshape(shape)(embed1)\n",
    "decoded1=decoder(embed1)\n",
    "output1=Conv2DTranspose(1,3,padding='same')(decoded1)\n",
    "\n",
    "embed2=Dense(np.prod(shape),activation='relu')(input_embed)\n",
    "embed2=Reshape(shape)(embed2)\n",
    "decoded2=decoder(embed2)\n",
    "output2=Conv2DTranspose(1,3,padding='same')(decoded2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47430850",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode=Model(input_blend,z)\n",
    "encode.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f9d170",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_component1 = Input(shape=(height, width, 1))\n",
    "input_component2 = Input(shape=(height, width, 1))\n",
    "\n",
    "decode=Model([input_embed, input_component1, input_component2], [output1, output2])\n",
    "decode.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cc759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = encode(input_blend)\n",
    "re1, re2 = decode([encoding, input_component1, input_component2])\n",
    "\n",
    "vae = Model([input_blend, input_component1, input_component2], [re1, re2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf6ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e179a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "blends.shape, components.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97600b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_lossA = mse(input_component1, re1) + mse(input_component2, re2)\n",
    "mse_lossA = tf.reduce_mean(mse_lossA / 2, axis=(1, 2))\n",
    "mse_lossB = mse(input_component1, re2) + mse(input_component2, re1)\n",
    "mse_lossB = tf.reduce_mean(mse_lossB / 2, axis=(1, 2))\n",
    "mse_loss = tf.reduce_mean(tf.minimum(mse_lossA, mse_lossB)) * height * width\n",
    "vae.add_loss(mse_loss)\n",
    "vae.add_metric(mse_loss, 'mse_loss')\n",
    "\n",
    "kl_loss=tf.reduce_mean(-0.5*(1+z_log_sigma-tf.square(z_mean)-tf.exp(z_log_sigma)))*0.1\n",
    "vae.add_loss(kl_loss)\n",
    "vae.add_metric(kl_loss,'kl_loss')\n",
    "\n",
    "\n",
    "train_together = [train_blends, train_components[0], train_components[1]]\n",
    "test_together = [test_blends, test_components[0], test_components[1]]\n",
    "\n",
    "vae.compile(optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dead41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history=vae.fit(train_together, verbose = 1, epochs = 100, batch_size = batch_size, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0613ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histplot(history):\n",
    "    plt.rcParams[\"figure.figsize\"] = (10,7)\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.plot(epochs, loss, label = 'Training loss')\n",
    "    plt.plot(epochs, val_loss, label = 'Validation loss')\n",
    "    plt.axhline( y = min(val_loss), color = 'black', linestyle = 'dotted', label = f'Minimum Validation Loss: {min(val_loss):.4f}')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a8a587",
   "metadata": {},
   "outputs": [],
   "source": [
    "histplot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb186a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showimg(img, ax, title=None, vmin=0, vmax=1):\n",
    "    ax.imshow(img.squeeze(), vmin=vmin, vmax=vmax)\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "\n",
    "def summary_plot(n,inputs,decode,seed=1):\n",
    "    np.random.seed(seed)\n",
    "    idx = np.random.choice(len(inputs[0]), size=n, replace=False)\n",
    "    print(idx)\n",
    "    input_img = inputs[0][idx]\n",
    "    comp1 = inputs[1][idx]\n",
    "    comp2 = inputs[2][idx]\n",
    "    z = encode.predict(input_img)\n",
    "    output_img1, output_img2 = decode.predict([z, comp1, comp2])\n",
    "    fig, ax = plt.subplots(7, n, figsize=((3)*n,15))\n",
    "    for i in range(n):\n",
    "        ax[0, i].imshow(input_img[i], origin='lower', interpolation='nearest', vmin = 0, vmax = 0.75)\n",
    "        ax[0, i].set_title('Input Blend')\n",
    "        ax[1, i].imshow(comp1[i], origin='lower', interpolation='nearest', vmin = 0, vmax = 0.75 )\n",
    "        ax[1, i].set_title('Input Component 1')\n",
    "        ax[2, i].imshow(comp2[i], origin='lower', interpolation='nearest', vmin = 0, vmax = 0.75)\n",
    "        ax[2, i].set_title('Input Component 2')\n",
    "        ax[3, i].imshow(output_img1[i], origin='lower', interpolation='nearest', vmin = 0, vmax = 0.75)\n",
    "        ax[3, i].set_title('Reconstruction Component 1')\n",
    "        ax[3, i].set_xlabel(f'MSE: {mean_squared_error(comp1[i], output_img1[i]):.2f}, SSIM: {ssim(comp1[i], output_img1[i], channel_axis = 2):.2f}')\n",
    "        ax[4, i].imshow(output_img2[i], origin='lower', interpolation='nearest', vmin = 0, vmax = 0.75)\n",
    "        ax[4, i].set_title('Reconstruction Component 2')\n",
    "        ax[5, i].imshow((output_img1[i] + output_img2[i]) , origin='lower', interpolation='nearest', vmin = 0, vmax = 0.75)\n",
    "        ax[5, i].set_title('Reconstruction Blend')\n",
    "        ax[4, i].set_xlabel(f'MSE: {mean_squared_error(comp2[i], output_img2[i]):.2f}, SSIM: {ssim(comp2[i], output_img2[i], channel_axis = 2):.2f}')\n",
    "        ax[6, i].imshow(abs(input_img[i] -(output_img1[i] + output_img2[i])) , origin='lower', interpolation='nearest', vmin = 0, vmax = 0.75)\n",
    "        ax[6, i].set_title('Blend Difference')\n",
    "        plt.tight_layout()\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e605d075",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n=5\n",
    "chosen=summary_plot(n,test_together,decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d176e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ssim(inputs,decode):\n",
    "    ssim_comp1 = np.zeros(test_ngals)\n",
    "    ssim_comp2 = np.zeros(test_ngals)\n",
    "    ssim_blends = np.zeros(test_ngals)\n",
    "    input_img = inputs[0]\n",
    "    comp1 = inputs[1]\n",
    "    comp2 = inputs[2]\n",
    "    z = encode.predict(input_img)\n",
    "    output_img1, output_img2 = decode.predict([z, comp1, comp2])\n",
    "    for i in range(test_ngals):\n",
    "        ssim_comp1[i] = ssim(comp1[i], output_img1[i], channel_axis = 2)\n",
    "        ssim_comp2[i] = ssim(comp2[i], output_img2[i], channel_axis = 2)\n",
    "        ssim_blends[i] = ssim(input_img[i], (output_img1[i] + output_img2[i]), channel_axis = 2)\n",
    "    mean_ssim_comp1 = np.mean(ssim_comp1)\n",
    "    median_ssim_comp1 = np.median(ssim_comp1)\n",
    "    \n",
    "    mean_ssim_comp2 = np.mean(ssim_comp2)\n",
    "    median_ssim_comp2 = np.median(ssim_comp2)\n",
    "    \n",
    "    mean_ssim_blends = np.mean(ssim_blends)\n",
    "    median_ssim_blends = np.median(ssim_blends)\n",
    "    \n",
    "    ssim_values = np.array([mean_ssim_comp1, median_ssim_comp1, mean_ssim_comp2, median_ssim_comp2, mean_ssim_blends, median_ssim_blends])\n",
    "    \n",
    "    return ssim_values \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dda3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_values = calc_ssim(test_together, decode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e6b5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ssim_comp1 = ssim_values[0]\n",
    "median_ssim_comp1 = ssim_values[1]\n",
    "mean_ssim_comp2 = ssim_values[2]\n",
    "median_ssim_comp2 = ssim_values[3]\n",
    "mean_ssim_blends = ssim_values[4]\n",
    "median_ssim_blends = ssim_values[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e841da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' Mean SSIMs: Component 1: {mean_ssim_comp1:.4f}, Component 2: {mean_ssim_comp2:.4f}, Blends: {mean_ssim_blends:.4f}')\n",
    "\n",
    "print(f' Median SSIMs: Component 1: {median_ssim_comp1:.4f}, Component 2: {median_ssim_comp2:.4f}, Blends: {median_ssim_blends:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f1e145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function calculates the properties of the input and output images. The ellipticity and flux are stored in arrays \n",
    "\n",
    "def calc_properties(inputs,decode):\n",
    "    \n",
    "    empty_array = np.zeros(test_ngals)\n",
    "    \n",
    "    flux_in_comp1 = np.zeros(test_ngals)\n",
    "    flux_in_comp2 = np.zeros(test_ngals)\n",
    "    flux_in_blends = np.zeros(test_ngals)\n",
    "    \n",
    "    flux_out_comp1 = np.zeros(test_ngals)\n",
    "    flux_out_comp2 = np.zeros(test_ngals)\n",
    "    flux_out_blends = np.zeros(test_ngals)\n",
    "    \n",
    "    \n",
    "    elip_in_comp1 = np.zeros(test_ngals)\n",
    "    elip_in_comp2 = np.zeros(test_ngals)\n",
    "    \n",
    "    elip_out_comp1 = np.zeros(test_ngals)\n",
    "    elip_out_comp2 = np.zeros(test_ngals)\n",
    "    \n",
    "    input_img = inputs[0]\n",
    "    input_img = input_img.reshape(test_ngals, height, width)\n",
    "    comp1 = inputs[1]\n",
    "    comp1 = comp1.reshape(test_ngals, height, width)\n",
    "    comp2 = inputs[2]\n",
    "    comp2 = comp2.reshape(test_ngals, height,width)\n",
    "    z = encode.predict(input_img)\n",
    "    output_img1, output_img2 = decode.predict([z, comp1, comp2])\n",
    "    \n",
    "    output_img1 = output_img1.reshape(test_ngals, height, width)\n",
    "    output_img2 = output_img2.reshape(test_ngals, height, width)\n",
    "    \n",
    "    for i in range(test_ngals):\n",
    "        \n",
    "        in_comp1_cat = photutils.morphology.data_properties(comp1[i], mask = None, background = 0)\n",
    "        elip_in_comp1[i] = in_comp1_cat.ellipticity\n",
    "        flux_in_comp1[i] = in_comp1_cat.segment_flux\n",
    "        \n",
    "        in_comp2_cat = photutils.morphology.data_properties(comp2[i], mask = None, background = 0)\n",
    "  \n",
    "        elip_in_comp2[i] = in_comp2_cat.ellipticity\n",
    "        flux_in_comp2[i] = in_comp2_cat.segment_flux\n",
    "        \n",
    "        in_blends_cat = photutils.morphology.data_properties(input_img[i], mask = None, background = 0)\n",
    "        flux_in_blends[i] = in_blends_cat.segment_flux\n",
    "\n",
    "            \n",
    "        out_comp1_cat = photutils.morphology.data_properties(output_img1[i], mask = None, background = 0)\n",
    "        elip_out_comp1[i] = out_comp1_cat.ellipticity\n",
    "        flux_out_comp1[i] = out_comp1_cat.segment_flux\n",
    "        \n",
    "        out_comp2_cat = photutils.morphology.data_properties(output_img2[i], mask = None, background = 0)\n",
    "        elip_out_comp2[i] = out_comp2_cat.ellipticity\n",
    "        flux_out_comp2[i] = out_comp2_cat.segment_flux\n",
    "        \n",
    "        out_blends_cat = photutils.morphology.data_properties((output_img1[i] + output_img2[i]), mask = None, background = 0)\n",
    "        flux_out_blends[i] = out_blends_cat.segment_flux\n",
    "        \n",
    "        parameters = [elip_in_comp1, elip_out_comp1, flux_in_comp1, flux_out_comp1, elip_in_comp2, elip_out_comp2, flux_in_comp2, flux_out_comp2, flux_in_blends, flux_out_blends]   \n",
    "    return parameters\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e410a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = calc_properties(test_together, decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47d2660",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ellip_in_comp1 = parameters[0]\n",
    "ellip_out_comp1 = parameters[1]\n",
    "flux_in_comp1 = parameters[2]\n",
    "flux_out_comp1 = parameters[3]\n",
    "ellip_in_comp2 = parameters[4]\n",
    "ellip_out_comp2 = parameters[5]\n",
    "flux_in_comp2 = parameters[6]\n",
    "flux_out_comp2 = parameters[7]\n",
    "flux_in_blends = parameters[8]\n",
    "flux_out_blends = parameters[9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775a0f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the standard deviations \n",
    "def calc_std(parameters):\n",
    "    num_of_params = len(parameters)\n",
    "    standard_dev_of_params = np.zeros(num_of_params)\n",
    "    for i in range(0,num_of_params):\n",
    "        standard_dev_of_params[i] = np.std(parameters[i])\n",
    "    return standard_dev_of_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7004b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_of_params = calc_std(parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ae915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Std dev of Ellip: Comp1 [In] {std_of_params[0]:.4f}, Comp1 [Out] {std_of_params[1]:.4f}, Comp2 [In] {std_of_params[4]:.4f}, Comp2 [Out] {std_of_params[5]:.4f} ')\n",
    "print(f'Std dev of Flux: Comp1 [In] {std_of_params[2]:.4f}, Comp1 [Out] {std_of_params[3]:.4f}, Comp2 [In] {std_of_params[6]:.4f}, Comp2 [Out] {std_of_params[7]:.4f}, Blend [In] {std_of_params[8]:.4f}, Blend [Out] {std_of_params[9]:.4f} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd551178",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize = (13,5))\n",
    "\n",
    "axes[0].set_title(\"Ellipticity of Component 1\")\n",
    "hex1 = axes[0].hexbin(ellip_in_comp1, ellip_out_comp1, gridsize=30, cmap='Blues', mincnt=10, alpha=0.7)\n",
    "axes[0].set_xlabel(\"Ellipticity of Component 1 [Input]\")\n",
    "axes[0].set_ylabel(\"Ellipticity of Component 1 [Output]\")\n",
    "fig.colorbar(hex1, ax=axes[0])\n",
    "\n",
    "lims = [\n",
    "    np.min([axes[0].get_xlim(), axes[0].get_ylim()]),  # min of both axes\n",
    "    np.max([axes[0].get_xlim(), axes[0].get_ylim()]),  # max of both axes\n",
    "]\n",
    "\n",
    "\n",
    "# now plot both limits against eachother\n",
    "axes[0].plot(lims, lims, 'k-', alpha=0.5, zorder=1)\n",
    "\n",
    "axes[1].set_title(\"Ellipticity of Component 2\")\n",
    "hex2 = axes[1].hexbin(ellip_in_comp2, ellip_out_comp2, gridsize=30, cmap='Blues', mincnt=10, alpha=0.7)\n",
    "axes[1].set_xlabel(\"Ellipticity Component 2 [Input]\")\n",
    "axes[1].set_ylabel(\"Ellipticity Component 2 [Output]\")\n",
    "fig.colorbar(hex2, ax=axes[1])\n",
    "\n",
    "lims = [\n",
    "    np.min([axes[1].get_xlim(), axes[1].get_ylim()]),  # min of both axes\n",
    "    np.max([axes[1].get_xlim(), axes[1].get_ylim()]),  # max of both axes\n",
    "]\n",
    "\n",
    "# now plot both limits against eachother\n",
    "axes[1].plot(lims, lims, 'k-', alpha=0.5, zorder=1)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31da1f9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize = (17,5))\n",
    "\n",
    "axes[0].set_title(\"Segment Flux of Component 1\")\n",
    "hex1 = axes[0].hexbin(flux_in_comp1, flux_out_comp1, gridsize=30, cmap='Blues', mincnt=10, alpha=0.7)\n",
    "axes[0].set_xlabel(\"Segment Flux of Component 1 [Input]\")\n",
    "axes[0].set_ylabel(\"Segment Flux of Component 1 [Output]\")\n",
    "fig.colorbar(hex1, ax=axes[0])\n",
    "\n",
    "lims = [\n",
    "    np.min([axes[0].get_xlim(), axes[0].get_ylim()]),  # min of both axes\n",
    "    np.max([axes[0].get_xlim(), axes[0].get_ylim()]),  # max of both axes\n",
    "]\n",
    "\n",
    "\n",
    "# now plot both limits against eachother\n",
    "axes[0].plot(lims, lims, 'k-', alpha=0.5, zorder=1)\n",
    "\n",
    "axes[1].set_title(\"Segment Flux of Component 2\")\n",
    "hex2 = axes[1].hexbin(flux_in_comp2, flux_out_comp2, gridsize=30, cmap='Blues', mincnt=10, alpha=0.7)\n",
    "axes[1].set_xlabel(\"Segment Flux of Component 2 [Input]\")\n",
    "axes[1].set_ylabel(\"Segment Flux of Component 2 [Output]\")\n",
    "fig.colorbar(hex2, ax=axes[1])\n",
    "\n",
    "lims = [\n",
    "    np.min([axes[1].get_xlim(), axes[1].get_ylim()]),  # min of both axes\n",
    "    np.max([axes[1].get_xlim(), axes[1].get_ylim()]),  # max of both axes\n",
    "]\n",
    "\n",
    "# now plot both limits against eachother\n",
    "axes[1].plot(lims, lims, 'k-', alpha=0.5, zorder=1)\n",
    "\n",
    "axes[2].set_title(\"Segment Flux of Blend\")\n",
    "hex3 = axes[2].hexbin(flux_in_blends, flux_out_blends, gridsize=30, cmap='Blues', mincnt=10, alpha=0.7)\n",
    "axes[2].set_xlabel(\"Segment Flux of Blend [Input]\")\n",
    "axes[2].set_ylabel(\"Segment Flux of Blend [Output]\")\n",
    "fig.colorbar(hex3, ax=axes[2])\n",
    "\n",
    "lims = [\n",
    "    np.min([axes[2].get_xlim(), axes[2].get_ylim()]),  # min of both axes\n",
    "    np.max([axes[2].get_xlim(), axes[2].get_ylim()]),  # max of both axes\n",
    "]\n",
    "\n",
    "# now plot both limits against eachother\n",
    "axes[2].plot(lims, lims, 'k-', alpha=0.5, zorder=1)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03cf242",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate accuracys \n",
    "\n",
    "ellip_comp1_accuracy = abs(ellip_out_comp1 / ellip_in_comp1) * 100\n",
    "mean_ellip_comp1_accuracy = np.mean(ellip_comp1_accuracy)\n",
    "median_ellip_comp1_accuracy = np.median(ellip_comp1_accuracy)\n",
    "\n",
    "flux_comp1_accuracy = abs(flux_out_comp1 / flux_in_comp1) * 100\n",
    "mean_flux_comp1_accuracy = np.mean(flux_comp1_accuracy)\n",
    "median_flux_comp1_accuracy = np.median(flux_comp1_accuracy)\n",
    "\n",
    "ellip_comp2_accuracy = abs(ellip_out_comp2 / ellip_in_comp2) * 100\n",
    "mean_ellip_comp2_accuracy = np.mean(ellip_comp2_accuracy)\n",
    "median_ellip_comp2_accuracy = np.median(ellip_comp2_accuracy)\n",
    "\n",
    "flux_comp2_accuracy = abs(flux_out_comp2 / flux_in_comp2) * 100\n",
    "mean_flux_comp2_accuracy = np.mean(flux_comp2_accuracy)\n",
    "median_flux_comp2_accuracy = np.median(flux_comp2_accuracy)\n",
    "\n",
    "flux_blend_accuracy = abs(flux_out_blends / flux_in_blends) * 100 \n",
    "mean_flux_blend_accuracy = np.mean(flux_blend_accuracy)\n",
    "median_flux_blend_accuracy = np.median(flux_blend_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021e62d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' Mean ellipticity accuracys: Component 1: {mean_ellip_comp1_accuracy :.4f}, Component 2: {mean_ellip_comp2_accuracy:.4f}')\n",
    "print(f' Median ellipticity accuracys: Component 1: {median_ellip_comp1_accuracy:.4f}, Component 2: {median_ellip_comp2_accuracy:.4f} \\n')\n",
    "\n",
    "print(f' Mean flux accuracys: Component 1: {mean_flux_comp1_accuracy:.4f}, Component 2: {mean_flux_comp2_accuracy:.4f}, Blends: {mean_flux_blend_accuracy:.4f}')\n",
    "print(f' Median flux accuracys: Component 1: {median_flux_comp1_accuracy:.4f}, Component 2: {median_flux_comp2_accuracy:.4f}, Blends: {median_flux_blend_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b24c2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4f878e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
